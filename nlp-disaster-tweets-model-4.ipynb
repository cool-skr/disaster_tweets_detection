{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":75138,"databundleVersionId":8247235,"sourceType":"competition"}],"dockerImageVersionId":30685,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Load data\ndata = pd.read_csv('/kaggle/input/nlp-disaster-tweets/train.csv')\n\n# Data preprocessing\ndata = data[['text', 'target']].dropna()\nX = data['text']\ny = data['target']\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Load pre-trained BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Tokenize input data\ntrain_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\ntest_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n\n# Create PyTorch DataLoader\ntrain_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n                              torch.tensor(train_encodings['attention_mask']),\n                              torch.tensor(y_train.tolist()))\ntest_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n                             torch.tensor(test_encodings['attention_mask']),\n                             torch.tensor(y_test.tolist()))\n\ntrain_sampler = RandomSampler(train_dataset)\ntrain_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n\ntest_sampler = SequentialSampler(test_dataset)\ntest_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n\n# Set up optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nepochs = 3\ntotal_steps = len(train_loader) * epochs\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\n# Training loop\nmodel.train()\nfor epoch in range(epochs):\n    for batch in train_loader:\n        input_ids, attention_mask, labels = batch\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n# Evaluation\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions.extend(logits.argmax(dim=1).tolist())\n        true_labels.extend(labels.tolist())\n\n# Calculate accuracy\naccuracy = accuracy_score(true_labels, predictions)\nprint(f'BERT Accuracy: {accuracy:.2f}')\n\n# Classification report\nreport = classification_report(true_labels, predictions)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T02:46:04.688233Z","iopub.execute_input":"2024-04-17T02:46:04.688593Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load test data\ntest_data = pd.read_csv('/kaggle/input/nlp-disaster-tweets/test.csv')\ntest_data = test_data[['text']].dropna()\nX_test = test_data['text']\n\n# Tokenize input data\ntest_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n\n# Create PyTorch DataLoader\ntest_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n                             torch.tensor(test_encodings['attention_mask']))\nbatch_size = 16\ntest_sampler = SequentialSampler(test_dataset)\ntest_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n\n# Load trained model\nmodel.eval()\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask = batch\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions.extend(logits.argmax(dim=1).tolist())\n\n# Save predictions to CSV file\ndf_predictions = pd.DataFrame({'id': test_data['id'], 'target': predictions})\ndf_predictions.to_csv('predictions_6.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}